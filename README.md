
# ğŸ§  Deepfake Detection Using GenConViT and FreqNet with Ensemble-Based Web Inference

> Ensemble-based deepfake detection system powered by GenConViT, FreqNet, and Vision Transformer. Includes a web-based inference interface using Flask for real-time image/video verification.

---

## ğŸ“¸ Project Overview

The system integrates:
- **GenConViT Autoencoder + VAE** (Generative Convolutional Vision Transformer)
- **FreqNet** (Frequency-Aware CNN)
- **ViT** (Vision Transformer from HuggingFace)

It combines **spatial**, **frequency**, and **latent representation** learning to classify media as `REAL` or `FAKE`.

---

## ğŸ–¼ï¸ System Architecture

### ğŸ“Š Fig. 1 â€“ Architecture Diagram
![Architecture Diagram](assets/fig1_architecture.png)

---

## ğŸŒ DetectApp Web Interface

DetectApp is a Flask-based frontend allowing users to upload images or videos and view authenticity predictions.

### ğŸ§­ Fig. 2 â€“ Landing Page with Carousel and Features
![Landing Page](assets/fig2_landing.png)

### ğŸ“¤ Fig. 3 â€“ Upload and About Section
![Upload Interface](assets/fig3_upload_about.png)

### ğŸ¯ Fig. 4 â€“ Result Page with Prediction Outcome
![Prediction Result](assets/fig4_result.png)

### ğŸ§® Fig. 5 â€“ Individual Model Predictions
![Model Predictions](assets/fig5_model_predictions.png)

---

## ğŸš€ Features

- âœ… Multi-model inference (GenConViT, FreqNet, ViT)
- ğŸ–¼ï¸ Image and Video support (`.jpg`, `.png`, `.mp4`, `.avi`)
- âš¡ Fast frame-level detection
- ğŸ” Secure file handling
- ğŸ“± Mobile-responsive web interface

---

## ğŸ› ï¸ Installation Instructions

### 2. Install Dependencies

Create a virtual environment and install all required dependencies:

```bash
# Create a virtual environment
python -m venv venv

# Activate the environment
# On macOS/Linux:
source venv/bin/activate

# On Windows:
venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

---

### 4. Run the Web App

After installing the dependencies and activating the virtual environment:

```bash
python app.py
```

The app will be available at: [http://127.0.0.1:5000](http://127.0.0.1:5000)

---

## ğŸ“‚ Folder Structure

```
.
â”œâ”€â”€ app.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ genconvit_ae.pth
â”‚   â”œâ”€â”€ genconvit_vae.pth
â”‚   â””â”€â”€ freqnet.pth
â”œâ”€â”€ static/uploads/
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ upload.html
â”‚   â””â”€â”€ result.html
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ fig1_architecture.png
â”‚   â”œâ”€â”€ fig2_landing.png
â”‚   â”œâ”€â”€ fig3_upload_about.png
â”‚   â”œâ”€â”€ fig4_result.png
â”‚   â””â”€â”€ fig5_model_predictions.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Dataset Used

### ğŸ”Š Deepfake-TIMIT (Video-Based)

- **Total Videos**: 10,240  
  - 320 real  
  - 9,920 deepfakes  
- **Frame Rate**: 25 frames per second  
- **Video Duration**: ~3 seconds each  
- **Total Image Frames**: ~768,000 frames

---

## âœ¨ Results and Evaluation

| Model       | Dataset Scope            | Accuracy     |
|-------------|---------------------------|--------------|
| GenConViT   | In-domain (video)         | ~98%         |
| FreqNet     | Cross-GAN (17 generators) | 92.8% (mean) |
| ViT         | Independent image dataset | ~70%         |

---

## ğŸ§  Model Inference Summary

| Model           | Input Type | Method             | Output      |
|------------------|------------|--------------------|-------------|
| GenConViT-AE     | Image/Video | Reconstruction     | Real/Fake   |
| GenConViT-VAE    | Image/Video | Latent Analysis    | Real/Fake   |
| FreqNet          | Image/Video | FFT-based Learning | Real/Fake   |
| ViT              | Image       | Transformer-Based  | Real/Fake   |

### Ensemble Voting
- Majority voting across 4 models.
- Confidence is averaged from all model outputs.

---

## ğŸ”® Future Enhancements

- â± Replace subprocess-based inference with native PyTorch pipeline
- ğŸ¥ Extend ViT to process video frames
- ğŸ Add 3D-CNN or temporal transformer for sequence consistency
- ğŸ§  Use Grad-CAM / saliency maps for interpretability

---

## ğŸ“„ License

This project is open-source and licensed under the MIT License.

---

## ğŸ‘¨â€ğŸ’» Authors

- Prince Raj  
- Sumit Dohan  
- Jatin Mudiraj  
- Aksshay Mathew P  
_CSE, IIT Ropar_

---

## ğŸŒ Acknowledgements

- [GenConViT Paper](https://arxiv.org/abs/2307.07036)  
- [FreqNet Paper](https://arxiv.org/abs/2403.07240)  
- [Hugging Face ViT Model](https://huggingface.co/google/vit-base-patch16-224-in21k)
